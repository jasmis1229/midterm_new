{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuiN4kHRKxiVX5oEiwBhG3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasmis1229/midterm_new/blob/main/%ED%8A%B8%EB%A0%8C%EB%93%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ CBOW Ïã§Ìóò Ï¥àÍ∏∞Ìôî\n",
        "\n",
        "# 1. ÎùºÏù¥Î∏åÎü¨Î¶¨ Î∂àÎü¨Ïò§Í∏∞\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import re\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# 2. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
        "df = pd.read_csv('/content/Ïã†Ï°∞Ïñ¥_Ïä§ÌÉÄÏùº_Ìå®ÌÑ¥_ÏôÑÏÑ±Î≥∏_UTF8SIG.csv')  # ÌååÏùºÎ™ÖÏùÄ ÌôòÍ≤ΩÏóê ÎßûÍ≤å ÏàòÏ†ï"
      ],
      "metadata": {
        "id": "Wd1t0ZUu6EXC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ CBOW Îç∞Ïù¥ÌÑ∞ Ï†ïÏ†ú\n",
        "\n",
        "# Ïã†Ï°∞Ïñ¥ Ïïû Î≤àÌò∏ Ï†úÍ±∞ Ìï®Ïàò\n",
        "def clean_word(word):\n",
        "    word = str(word)\n",
        "    return re.sub(r'^\\d+\\.', '', word).strip()\n",
        "\n",
        "# ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥ Ïª¨Îüº ÏÉùÏÑ±\n",
        "df['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'] = df['Ïã†Ï°∞Ïñ¥'].apply(clean_word)"
      ],
      "metadata": {
        "id": "yqI1uxJtPJlU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ CBOW ÏÑ§Î™Ö ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï\n",
        "\n",
        "# Í∞ÑÎã® ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï Ìï®Ïàò\n",
        "def simple_tokenize(text):\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", str(text))  # ÌäπÏàòÎ¨∏Ïûê Ï†úÍ±∞\n",
        "    tokens = text.strip().split()\n",
        "    tokens = [token for token in tokens if len(token) > 1]  # Ìïú Í∏ÄÏûê Ï†úÍ±∞\n",
        "    return tokens\n",
        "\n",
        "# ÏÑ§Î™Ö ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï Ï†ÅÏö©\n",
        "descriptions = df['ÏÑ§Î™Ö'].tolist()\n",
        "tokenized_descriptions = [simple_tokenize(desc) for desc in descriptions]"
      ],
      "metadata": {
        "id": "lFWro2mUPbUc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ CBOW Ïñ¥Ìúò ÏÇ¨Ï†Ñ Íµ¨Ï∂ï\n",
        "\n",
        "vocab = set()\n",
        "for tokens in tokenized_descriptions:\n",
        "    vocab.update(tokens)\n",
        "\n",
        "# Îã®Ïñ¥ ‚Üî Ïù∏Îç±Ïä§ Îß§Ìïë\n",
        "vocab = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in vocab.items()}\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "TVTD5BY_Puyt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ CBOW ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±\n",
        "\n",
        "window_size = 2\n",
        "context_center_pairs = []\n",
        "\n",
        "for tokens in tokenized_descriptions:\n",
        "    for idx, center_word in enumerate(tokens):\n",
        "        context = []\n",
        "        for i in range(idx - window_size, idx + window_size + 1):\n",
        "            if i != idx and 0 <= i < len(tokens):\n",
        "                context.append(vocab[tokens[i]])\n",
        "        if context:\n",
        "            context_center_pairs.append((context, vocab[center_word]))"
      ],
      "metadata": {
        "id": "qNnXiZirQAt0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ CBOW Dataset ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
        "\n",
        "class CBOWDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context, center = self.data[idx]\n",
        "        return torch.tensor(context), torch.tensor(center)\n",
        "\n",
        "# DataLoader ÏÉùÏÑ±\n",
        "dataset = CBOWDataset(context_center_pairs)\n",
        "train_loader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=lambda batch: list(zip(*batch)))"
      ],
      "metadata": {
        "id": "n_HCqFoaQQgc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ CBOW Î™®Îç∏ ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
        "\n",
        "class CBOWModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100):\n",
        "        super(CBOWModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, contexts):\n",
        "        embeds = []\n",
        "        for context in contexts:\n",
        "            embed = self.embeddings(context)\n",
        "            embeds.append(embed.mean(dim=0))\n",
        "        embeds = torch.stack(embeds)\n",
        "        out = self.linear(embeds)\n",
        "        return out"
      ],
      "metadata": {
        "id": "SKDHKfjxQgG6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ CBOW Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
        "\n",
        "model = CBOWModel(vocab_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "HGwlT_bvQwG6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ CBOW ÌïôÏäµ Î£®ÌîÑ\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for contexts, centers in train_loader:\n",
        "        outputs = model(contexts)\n",
        "        loss = criterion(outputs, torch.tensor(centers))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jTFVCgLRIJq",
        "outputId": "8c866f54-fee4-4281-f6bc-606008287200"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 102.8183\n",
            "Epoch [2/20], Loss: 99.6956\n",
            "Epoch [3/20], Loss: 96.8455\n",
            "Epoch [4/20], Loss: 94.1754\n",
            "Epoch [5/20], Loss: 91.3577\n",
            "Epoch [6/20], Loss: 88.4945\n",
            "Epoch [7/20], Loss: 85.8633\n",
            "Epoch [8/20], Loss: 83.0899\n",
            "Epoch [9/20], Loss: 80.3510\n",
            "Epoch [10/20], Loss: 77.5348\n",
            "Epoch [11/20], Loss: 74.8449\n",
            "Epoch [12/20], Loss: 72.0077\n",
            "Epoch [13/20], Loss: 69.2950\n",
            "Epoch [14/20], Loss: 66.4486\n",
            "Epoch [15/20], Loss: 63.9486\n",
            "Epoch [16/20], Loss: 61.0662\n",
            "Epoch [17/20], Loss: 58.3412\n",
            "Epoch [18/20], Loss: 55.7727\n",
            "Epoch [19/20], Loss: 53.1345\n",
            "Epoch [20/20], Loss: 50.5563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ïó¨Í∏∞ÍπåÏßÄÍ∞Ä CBOW ÌïôÏäµ**"
      ],
      "metadata": {
        "id": "fBDGQqFCRb-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ CBOW Î¨∏Îß• ÏòàÏ∏° ÌÖåÏä§Ìä∏\n",
        "\n",
        "model.eval()\n",
        "\n",
        "random_idx = random.randint(0, len(context_center_pairs) - 1)\n",
        "random_context, true_center = context_center_pairs[random_idx]\n",
        "\n",
        "context_tensor = torch.tensor([random_context])\n",
        "predicted_logits = model(context_tensor)\n",
        "predicted_probs = F.softmax(predicted_logits, dim=1)\n",
        "top5 = torch.topk(predicted_probs, 5)\n",
        "\n",
        "print(\"\\n[CBOW ÏòàÏ∏° ÌÖåÏä§Ìä∏ Í≤∞Í≥º]\")\n",
        "print(f\"Î¨∏Îß•(Context): {[idx_to_word[idx] for idx in random_context]}\")\n",
        "print(f\"Ï†ïÎãµ(Center): {idx_to_word[true_center]}\")\n",
        "\n",
        "print(\"\\nTop-5 ÏòàÏ∏° Îã®Ïñ¥:\")\n",
        "for i in range(5):\n",
        "    idx = top5.indices[0][i].item()\n",
        "    prob = top5.values[0][i].item()\n",
        "    print(f\"{i+1}. {idx_to_word[idx]} (ÌôïÎ•†: {prob:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcyYJdduR5bb",
        "outputId": "5f9f4d3a-4370-4cec-b790-e672b95803e4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[CBOW ÏòàÏ∏° ÌÖåÏä§Ìä∏ Í≤∞Í≥º]\n",
            "Î¨∏Îß•(Context): ['Ïó¨ÌñâÏùÑ', 'Í∞ÄÎì†', 'ÌïòÎì†', 'ÏùºÏïäÌïòÍ≥†']\n",
            "Ï†ïÎãµ(Center): Î≠îÏùºÏùÑ\n",
            "\n",
            "Top-5 ÏòàÏ∏° Îã®Ïñ¥:\n",
            "1. Î≠îÏùºÏùÑ (ÌôïÎ•†: 0.0135)\n",
            "2. ÏÜêÍ∞ÄÎùΩÏùÄ (ÌôïÎ•†: 0.0085)\n",
            "3. ÌïòÎì† (ÌôïÎ•†: 0.0037)\n",
            "4. ÏïäÎèÑÎ°ù (ÌôïÎ•†: 0.0029)\n",
            "5. Í≤åÏãúÎ¨º (ÌôïÎ•†: 0.0026)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ CBOW ÏòàÏ∏° ÏµúÏ†ÅÌôî\n",
        "\n",
        "model.eval()\n",
        "\n",
        "random_idx = random.randint(0, len(context_center_pairs) - 1)\n",
        "random_context, true_center = context_center_pairs[random_idx]\n",
        "\n",
        "with torch.no_grad():\n",
        "    context_tensor = torch.tensor([random_context])\n",
        "    predicted_logits = model(context_tensor)\n",
        "    predicted_probs = F.softmax(predicted_logits, dim=1)\n",
        "    top5 = torch.topk(predicted_probs, 5)\n",
        "\n",
        "print(\"\\n[CBOW ÏòàÏ∏° ÌÖåÏä§Ìä∏ Í≤∞Í≥º Í∞úÏÑ†]\")\n",
        "print(f\"Î¨∏Îß•(Context): {[idx_to_word[idx] for idx in random_context]}\")\n",
        "print(f\"Ï†ïÎãµ(Center): {idx_to_word[true_center]}\")\n",
        "\n",
        "print(\"\\nTop-5 ÏòàÏ∏° Îã®Ïñ¥:\")\n",
        "for i in range(5):\n",
        "    idx = top5.indices[0][i].item()\n",
        "    prob = top5.values[0][i].item()\n",
        "    print(f\"{i+1}. {idx_to_word[idx]} (ÌôïÎ•†: {prob:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr5yxnxwSVsc",
        "outputId": "29a0fdb6-dfd7-4d22-890b-c6d1ea4b7624"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[CBOW ÏòàÏ∏° ÌÖåÏä§Ìä∏ Í≤∞Í≥º Í∞úÏÑ†]\n",
            "Î¨∏Îß•(Context): ['Ïã∂Ïñ¥ÏÑú', 'ÎààÏïåÏù¥']\n",
            "Ï†ïÎãµ(Center): ÍºøÌûêÎïå\n",
            "\n",
            "Top-5 ÏòàÏ∏° Îã®Ïñ¥:\n",
            "1. ÍºøÌûêÎïå (ÌôïÎ•†: 0.0488)\n",
            "2. Î¨ºÍ±¥ (ÌôïÎ•†: 0.0101)\n",
            "3. ÏÇ¨Í≥† (ÌôïÎ•†: 0.0075)\n",
            "4. Îú∏Îì§Ïñ¥ (ÌôïÎ•†: 0.0072)\n",
            "5. ÎààÏïåÏù¥ (ÌôïÎ•†: 0.0072)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ CBOW Ïä§ÌÉÄÏùº Í∏∞Î∞ò ÎØ∏Îûò Ïã†Ï°∞Ïñ¥ ÏÉùÏÑ±\n",
        "\n",
        "sample_rows = df.sample(3, random_state=random.randint(0,10000))\n",
        "\n",
        "print(\"\\n[ÎØ∏Îûò Ïã†Ï°∞Ïñ¥ Ïä§ÌÉÄÏùº ÏòàÏ∏° Î∞è ÏÉùÏÑ±]\")\n",
        "\n",
        "selected_words = []\n",
        "combined_styles = set()\n",
        "\n",
        "for idx, row in sample_rows.iterrows():\n",
        "    word = row['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥']\n",
        "    style = row['Ïä§ÌÉÄÏùº Ìå®ÌÑ¥']\n",
        "    print(f\"Í∏∞Ï°¥ Ïã†Ï°∞Ïñ¥: {word}\")\n",
        "    print(f\"Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: {style}\\n\")\n",
        "    selected_words.append(word)\n",
        "    combined_styles.update(style.split(', '))\n",
        "\n",
        "def safe_pick(word, pos):\n",
        "    if len(word) == 1:\n",
        "        return word\n",
        "    if pos == 'start':\n",
        "        return word[0]\n",
        "    elif pos == 'middle':\n",
        "        return word[len(word)//2]\n",
        "    elif pos == 'end':\n",
        "        return word[-1]\n",
        "    else:\n",
        "        return random.choice(word)\n",
        "\n",
        "new_created_word = (\n",
        "    safe_pick(selected_words[0], 'start') +\n",
        "    safe_pick(selected_words[1], 'middle') +\n",
        "    safe_pick(selected_words[2], 'end')\n",
        ")\n",
        "\n",
        "print(f\"üëâ ÏÉùÏÑ±Îêú ÎØ∏Îûò Ïã†Ï°∞Ïñ¥: {new_created_word}\")\n",
        "print(f\"üëâ ÏòàÏÉÅÎêòÎäî Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: {', '.join(list(combined_styles))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFgcHN-qSomZ",
        "outputId": "362caf0e-6d9e-40ec-b2d0-8d11a0855df0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ÎØ∏Îûò Ïã†Ï°∞Ïñ¥ Ïä§ÌÉÄÏùº ÏòàÏ∏° Î∞è ÏÉùÏÑ±]\n",
            "Í∏∞Ï°¥ Ïã†Ï°∞Ïñ¥: ÎπÑÎã¥\n",
            "Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: Î¶¨Îì¨Í∞ê\n",
            "\n",
            "Í∏∞Ï°¥ Ïã†Ï°∞Ïñ¥: ÏûêÎùºÏ°±\n",
            "Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: Î¶¨Îì¨Í∞ê+Í∞êÏ†ïÏÑ±\n",
            "\n",
            "Í∏∞Ï°¥ Ïã†Ï°∞Ïñ¥: ÏÑ†Î¶¨ÌõÑÍ∞ê\n",
            "Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: Î¶¨Îì¨Í∞ê+Í∞êÏ†ïÏÑ±\n",
            "\n",
            "üëâ ÏÉùÏÑ±Îêú ÎØ∏Îûò Ïã†Ï°∞Ïñ¥: ÎπÑÎùºÍ∞ê\n",
            "üëâ ÏòàÏÉÅÎêòÎäî Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: Î¶¨Îì¨Í∞ê+Í∞êÏ†ïÏÑ±, Î¶¨Îì¨Í∞ê\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ï≤´ ÏãúÎèÑ ÏΩîÎìú"
      ],
      "metadata": {
        "id": "4nsOiz4OToNh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlIlPx1uF5P-",
        "outputId": "2757b691-e5ea-48d2-f857-10acf4954834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 102.7254\n",
            "Epoch [2/20], Loss: 99.6482\n",
            "Epoch [3/20], Loss: 96.8595\n",
            "Epoch [4/20], Loss: 94.0647\n",
            "Epoch [5/20], Loss: 91.2805\n",
            "Epoch [6/20], Loss: 88.6131\n",
            "Epoch [7/20], Loss: 85.7855\n",
            "Epoch [8/20], Loss: 83.0773\n",
            "Epoch [9/20], Loss: 80.2598\n",
            "Epoch [10/20], Loss: 77.5700\n",
            "Epoch [11/20], Loss: 74.9174\n",
            "Epoch [12/20], Loss: 72.1179\n",
            "Epoch [13/20], Loss: 69.3065\n",
            "Epoch [14/20], Loss: 66.5300\n",
            "Epoch [15/20], Loss: 63.8406\n",
            "Epoch [16/20], Loss: 61.1367\n",
            "Epoch [17/20], Loss: 58.4678\n",
            "Epoch [18/20], Loss: 55.9640\n",
            "Epoch [19/20], Loss: 53.1117\n",
            "Epoch [20/20], Loss: 50.5708\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ ColabÏö© CBOW ÎØ∏Îûò Ïã†Ï°∞Ïñ¥ Ïä§ÌÉÄÏùº ÏòàÏ∏°\n",
        "\n",
        "# 1. ÎùºÏù¥Î∏åÎü¨Î¶¨ Î∂àÎü¨Ïò§Í∏∞\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 2. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞ (ColabÏóê ÏóÖÎ°úÎìúÌïú ÌååÏùº ÏßÅÏ†ë Í≤ΩÎ°úÎ°ú ÏùΩÍ∏∞)\n",
        "df = pd.read_csv('/content/Ïã†Ï°∞Ïñ¥_Îç∞Ïù¥ÌÑ∞ÏÖã_ÏµúÏ¢ÖÎ≥∏.csv')\n",
        "\n",
        "# 3. Í∞ÑÎã® ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï Ìï®Ïàò\n",
        "def simple_tokenize(text):\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", str(text))  # ÌäπÏàòÎ¨∏Ïûê Ï†úÍ±∞\n",
        "    tokens = text.strip().split()\n",
        "    tokens = [token for token in tokens if len(token) > 1]  # Ìïú Í∏ÄÏûê Ï†úÍ±∞\n",
        "    return tokens\n",
        "\n",
        "# 4. Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨: ÏÑ§Î™ÖÏùÑ ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï\n",
        "descriptions = df['ÏÑ§Î™Ö'].tolist()\n",
        "tokenized_descriptions = [simple_tokenize(desc) for desc in descriptions]\n",
        "\n",
        "# 5. Ïñ¥Ìúò ÏÇ¨Ï†Ñ Íµ¨Ï∂ï\n",
        "vocab = set()\n",
        "for tokens in tokenized_descriptions:\n",
        "    vocab.update(tokens)\n",
        "vocab = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in vocab.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# 6. CBOW ÌïôÏäµÏö© Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±\n",
        "window_size = 2\n",
        "context_center_pairs = []\n",
        "\n",
        "for tokens in tokenized_descriptions:\n",
        "    for idx, center_word in enumerate(tokens):\n",
        "        context = []\n",
        "        for i in range(idx - window_size, idx + window_size + 1):\n",
        "            if i != idx and 0 <= i < len(tokens):\n",
        "                context.append(vocab[tokens[i]])\n",
        "        if context:\n",
        "            context_center_pairs.append((context, vocab[center_word]))\n",
        "\n",
        "# 7. Dataset ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
        "class CBOWDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context, center = self.data[idx]\n",
        "        return torch.tensor(context), torch.tensor(center)\n",
        "\n",
        "# 8. DataLoader ÏÉùÏÑ±\n",
        "dataset = CBOWDataset(context_center_pairs)\n",
        "train_loader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=lambda batch: list(zip(*batch)))\n",
        "\n",
        "# 9. CBOW Î™®Îç∏ Ï†ïÏùò\n",
        "class CBOWModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100):\n",
        "        super(CBOWModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, contexts):\n",
        "        embeds = []\n",
        "        for context in contexts:\n",
        "            embed = self.embeddings(context)\n",
        "            embeds.append(embed.mean(dim=0))\n",
        "        embeds = torch.stack(embeds)\n",
        "        out = self.linear(embeds)\n",
        "        return out\n",
        "\n",
        "# 10. Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
        "model = CBOWModel(vocab_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 11. ÌïôÏäµ Î£®ÌîÑ\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for contexts, centers in train_loader:\n",
        "        outputs = model(contexts)\n",
        "        loss = criterion(outputs, torch.tensor(centers))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}')\n",
        "\n",
        "# 12. ÌïôÏäµ ÏôÑÎ£å!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Í∞úÏÑ†"
      ],
      "metadata": {
        "id": "InfP-QdoT64G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ ColabÏö© CBOW ÎØ∏Îûò Ïã†Ï°∞Ïñ¥ Ïä§ÌÉÄÏùº ÏòàÏ∏° Í∏∞Î≥∏ ÏΩîÎìú (ÏòàÏ∏° ÌÖåÏä§Ìä∏ Ï∂îÍ∞Ä)\n",
        "\n",
        "# 1. ÎùºÏù¥Î∏åÎü¨Î¶¨ Î∂àÎü¨Ïò§Í∏∞\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import re\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 2. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞ (ColabÏóê ÏóÖÎ°úÎìúÌïú ÌååÏùº ÏßÅÏ†ë Í≤ΩÎ°úÎ°ú ÏùΩÍ∏∞)\n",
        "df = pd.read_csv('/content/Ïã†Ï°∞Ïñ¥_Îç∞Ïù¥ÌÑ∞ÏÖã_ÏµúÏ¢ÖÎ≥∏_ÎûúÎç§ÎùºÎ≤®ÎßÅ.csv')\n",
        "\n",
        "# 3. Í∞ÑÎã® ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï Ìï®Ïàò\n",
        "def simple_tokenize(text):\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", str(text))  # ÌäπÏàòÎ¨∏Ïûê Ï†úÍ±∞\n",
        "    tokens = text.strip().split()\n",
        "    tokens = [token for token in tokens if len(token) > 1]  # Ìïú Í∏ÄÏûê Ï†úÍ±∞\n",
        "    return tokens\n",
        "\n",
        "# 4. Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨: ÏÑ§Î™ÖÏùÑ ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï\n",
        "descriptions = df['ÏÑ§Î™Ö'].tolist()\n",
        "tokenized_descriptions = [simple_tokenize(desc) for desc in descriptions]\n",
        "\n",
        "# 5. Ïñ¥Ìúò ÏÇ¨Ï†Ñ Íµ¨Ï∂ï\n",
        "vocab = set()\n",
        "for tokens in tokenized_descriptions:\n",
        "    vocab.update(tokens)\n",
        "vocab = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in vocab.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# 6. CBOW ÌïôÏäµÏö© Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±\n",
        "window_size = 2\n",
        "context_center_pairs = []\n",
        "\n",
        "for tokens in tokenized_descriptions:\n",
        "    for idx, center_word in enumerate(tokens):\n",
        "        context = []\n",
        "        for i in range(idx - window_size, idx + window_size + 1):\n",
        "            if i != idx and 0 <= i < len(tokens):\n",
        "                context.append(vocab[tokens[i]])\n",
        "        if context:\n",
        "            context_center_pairs.append((context, vocab[center_word]))\n",
        "\n",
        "# 7. Dataset ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
        "class CBOWDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context, center = self.data[idx]\n",
        "        return torch.tensor(context), torch.tensor(center)\n",
        "\n",
        "# 8. DataLoader ÏÉùÏÑ±\n",
        "dataset = CBOWDataset(context_center_pairs)\n",
        "train_loader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=lambda batch: list(zip(*batch)))\n",
        "\n",
        "# 9. CBOW Î™®Îç∏ Ï†ïÏùò\n",
        "class CBOWModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100):\n",
        "        super(CBOWModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, contexts):\n",
        "        embeds = []\n",
        "        for context in contexts:\n",
        "            embed = self.embeddings(context)\n",
        "            embeds.append(embed.mean(dim=0))\n",
        "        embeds = torch.stack(embeds)\n",
        "        out = self.linear(embeds)\n",
        "        return out\n",
        "\n",
        "# 10. Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
        "model = CBOWModel(vocab_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 11. ÌïôÏäµ Î£®ÌîÑ\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for contexts, centers in train_loader:\n",
        "        outputs = model(contexts)\n",
        "        loss = criterion(outputs, torch.tensor(centers))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}')\n",
        "\n",
        "# 12. ÏòàÏ∏° ÌÖåÏä§Ìä∏\n",
        "model.eval()\n",
        "\n",
        "# ÎûúÎç§ context-center ÌïòÎÇò ÎΩëÍ∏∞\n",
        "random_idx = random.randint(0, len(context_center_pairs) - 1)\n",
        "random_context, true_center = context_center_pairs[random_idx]\n",
        "\n",
        "# ÏòàÏ∏° ÏàòÌñâ\n",
        "context_tensor = torch.tensor([random_context])\n",
        "predicted_logits = model(context_tensor)\n",
        "predicted_probs = F.softmax(predicted_logits, dim=1)\n",
        "top5 = torch.topk(predicted_probs, 5)\n",
        "\n",
        "print(\"\\n[ÏòàÏ∏° ÌÖåÏä§Ìä∏ Í≤∞Í≥º]\")\n",
        "print(f\"Î¨∏Îß•(Context): {[idx_to_word[idx] for idx in random_context]}\")\n",
        "print(f\"Ï†ïÎãµ(Center): {idx_to_word[true_center]}\")\n",
        "\n",
        "print(\"\\nTop-5 ÏòàÏ∏° Îã®Ïñ¥:\")\n",
        "for i in range(5):\n",
        "    idx = top5.indices[0][i].item()\n",
        "    prob = top5.values[0][i].item()\n",
        "    print(f\"{i+1}. {idx_to_word[idx]} (ÌôïÎ•†: {prob:.4f})\")\n",
        "\n",
        "# 13. ÌïôÏäµ Î∞è ÌÖåÏä§Ìä∏ ÏôÑÎ£å!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGDGHqzFJ6eY",
        "outputId": "698aea9e-1fbd-4ca4-8773-d73424078025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 102.6710\n",
            "Epoch [2/20], Loss: 99.6654\n",
            "Epoch [3/20], Loss: 96.8958\n",
            "Epoch [4/20], Loss: 94.1144\n",
            "Epoch [5/20], Loss: 91.4124\n",
            "Epoch [6/20], Loss: 88.6321\n",
            "Epoch [7/20], Loss: 85.8246\n",
            "Epoch [8/20], Loss: 83.1512\n",
            "Epoch [9/20], Loss: 80.4493\n",
            "Epoch [10/20], Loss: 77.7554\n",
            "Epoch [11/20], Loss: 74.9357\n",
            "Epoch [12/20], Loss: 72.1357\n",
            "Epoch [13/20], Loss: 69.4429\n",
            "Epoch [14/20], Loss: 66.6867\n",
            "Epoch [15/20], Loss: 63.9483\n",
            "Epoch [16/20], Loss: 61.3349\n",
            "Epoch [17/20], Loss: 58.5307\n",
            "Epoch [18/20], Loss: 55.7705\n",
            "Epoch [19/20], Loss: 53.3817\n",
            "Epoch [20/20], Loss: 50.6909\n",
            "\n",
            "[ÏòàÏ∏° ÌÖåÏä§Ìä∏ Í≤∞Í≥º]\n",
            "Î¨∏Îß•(Context): ['ÎßàÏä§ÌÅ¨Î•º', 'Î≤óÏóàÏùÑÎïå', 'Î™®ÏäµÏù¥']\n",
            "Ï†ïÎãµ(Center): ÏçºÏùÑÎïåÏôÄ\n",
            "\n",
            "Top-5 ÏòàÏ∏° Îã®Ïñ¥:\n",
            "1. ÏçºÏùÑÎïåÏôÄ (ÌôïÎ•†: 0.0142)\n",
            "2. Î≤óÏóàÏùÑÎïå (ÌôïÎ•†: 0.0033)\n",
            "3. ÎßàÏä§ÌÅ¨Î•º (ÌôïÎ•†: 0.0032)\n",
            "4. ÎçîÎüΩÎã§ (ÌôïÎ•†: 0.0031)\n",
            "5. ÎπÑÏä∑Ìï®ÏùÑ (ÌôïÎ•†: 0.0030)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ïä§ÌÉÄÏùº Ìå®ÌÑ¥ ÎèôÏùºÌïòÍ≤å Ïã§Ìñâ(Ïã§Ìå®)"
      ],
      "metadata": {
        "id": "d-t2CiTfUFa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ÎùºÏù¥Î∏åÎü¨Î¶¨ Î∂àÎü¨Ïò§Í∏∞\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import re\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# 2. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
        "df = pd.read_csv('/content/Ïã†Ï°∞Ïñ¥_Îç∞Ïù¥ÌÑ∞ÏÖã_ÏµúÏ¢ÖÎ≥∏_ÎûúÎç§ÎùºÎ≤®ÎßÅ.csv')\n",
        "\n",
        "# 3. Î≤àÌò∏ Ï†úÍ±∞ Ìï®Ïàò\n",
        "def clean_word(word):\n",
        "    word = str(word)\n",
        "    return re.sub(r'^\\d+\\.', '', word).strip()\n",
        "\n",
        "# 4. Ïä§ÌÉÄÏùº Ìå®ÌÑ¥ ÏÉùÏÑ± Ìï®Ïàò (Ïà´ÏûêÎ¶¨Îì¨ Ï†úÍ±∞)\n",
        "def assign_style_in_code(word, desc):\n",
        "    styles = set()\n",
        "    if 2 <= len(word) <= 4:\n",
        "        styles.add('ÏßßÏùå')\n",
        "    if any(kw in word for kw in ['ÌÇπ', 'Ï°¥', 'Í∞ì', 'Îπ°', 'Ï≤†Ïªπ', 'Í∑π', 'Ï∞ê', 'ÍøÄ', 'ÎßõÌÉ±']):\n",
        "        styles.add('Í∞êÏ†ïÏÑ±')\n",
        "    if re.search(r'(..)\\1', word) or 'Î¥ê' in word or 'ÎÜà' in word:\n",
        "        styles.add('Î¶¨Îì¨Í∞ê')\n",
        "    if any(kw in desc for kw in ['ÌòÑÏã§', 'ÏÇ¨Î∞îÏÇ¨', 'Î≥µÏÑ∏Ìé∏ÏÇ¥', 'Ìé∏ÌïòÍ≤å', 'Î¨∏Ìôî', 'ÏßÑÏÉÅ', 'ÌòÑÏÉù']):\n",
        "        styles.add('ÌòÑÏã§ÏÑ±')\n",
        "    if any(kw in desc for kw in ['ÏõÉÍ∏¥', 'Î∞à', 'ÏõÉÏùå', 'Ìå®Îü¨Îîî', 'Ïú†Î®∏', 'Í∞úÍ∑∏']):\n",
        "        styles.add('Ïú†Î®∏ÏÑ±')\n",
        "    if any(kw in desc for kw in ['Ìè≠', 'ÌÖåÎü¨', 'Í≥µÍ≤©', 'ÎπÑÌïò', 'ÌòêÏò§']):\n",
        "        styles.add('Í≥ºÍ≤©ÏÑ±')\n",
        "    if not styles:\n",
        "        styles.add('ÏßßÏùå')\n",
        "    return ', '.join(styles)\n",
        "\n",
        "# 5. Ïã†Ï°∞Ïñ¥ ÌÅ¥Î¶∞ + Ïä§ÌÉÄÏùº Ï∂îÍ∞Ä\n",
        "df['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'] = df['Ïã†Ï°∞Ïñ¥'].apply(clean_word)\n",
        "df['Ïä§ÌÉÄÏùº Ìå®ÌÑ¥'] = df.apply(lambda row: assign_style_in_code(row['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'], row['ÏÑ§Î™Ö']), axis=1)\n",
        "\n",
        "# 6. Í∞ÑÎã® ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï\n",
        "def simple_tokenize(text):\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", str(text))\n",
        "    tokens = text.strip().split()\n",
        "    tokens = [token for token in tokens if len(token) > 1]\n",
        "    return tokens\n",
        "\n",
        "# 7. ÏÑ§Î™Ö ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï\n",
        "descriptions = df['ÏÑ§Î™Ö'].tolist()\n",
        "tokenized_descriptions = [simple_tokenize(desc) for desc in descriptions]\n",
        "\n",
        "# 8. Ïñ¥Ìúò ÏÇ¨Ï†Ñ Íµ¨Ï∂ï\n",
        "vocab = set()\n",
        "for tokens in tokenized_descriptions:\n",
        "    vocab.update(tokens)\n",
        "vocab = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in vocab.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# 9. CBOW ÌïôÏäµÏö© Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±\n",
        "window_size = 2\n",
        "context_center_pairs = []\n",
        "\n",
        "for tokens in tokenized_descriptions:\n",
        "    for idx, center_word in enumerate(tokens):\n",
        "        context = []\n",
        "        for i in range(idx - window_size, idx + window_size + 1):\n",
        "            if i != idx and 0 <= i < len(tokens):\n",
        "                context.append(vocab[tokens[i]])\n",
        "        if context:\n",
        "            context_center_pairs.append((context, vocab[center_word]))\n",
        "\n",
        "# 10. Dataset ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
        "class CBOWDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context, center = self.data[idx]\n",
        "        return torch.tensor(context), torch.tensor(center)\n",
        "\n",
        "# 11. DataLoader ÏÉùÏÑ±\n",
        "dataset = CBOWDataset(context_center_pairs)\n",
        "train_loader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=lambda batch: list(zip(*batch)))\n",
        "\n",
        "# 12. CBOW Î™®Îç∏ Ï†ïÏùò\n",
        "class CBOWModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100):\n",
        "        super(CBOWModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, contexts):\n",
        "        embeds = []\n",
        "        for context in contexts:\n",
        "            embed = self.embeddings(context)\n",
        "            embeds.append(embed.mean(dim=0))\n",
        "        embeds = torch.stack(embeds)\n",
        "        out = self.linear(embeds)\n",
        "        return out\n",
        "\n",
        "# 13. Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
        "model = CBOWModel(vocab_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 14. CBOW Î™®Îç∏ ÌïôÏäµ\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for contexts, centers in train_loader:\n",
        "        outputs = model(contexts)\n",
        "        loss = criterion(outputs, torch.tensor(centers))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}')\n",
        "\n",
        "# 15. CBOW Î¨∏Îß• ÏòàÏ∏° ÌÖåÏä§Ìä∏\n",
        "model.eval()\n",
        "\n",
        "# ÎûúÎç§ context-center ÌïòÎÇò ÎΩëÍ∏∞\n",
        "random_idx = random.randint(0, len(context_center_pairs) - 1)\n",
        "random_context, true_center = context_center_pairs[random_idx]\n",
        "\n",
        "# ÏòàÏ∏° ÏàòÌñâ\n",
        "context_tensor = torch.tensor([random_context])\n",
        "predicted_logits = model(context_tensor)\n",
        "predicted_probs = F.softmax(predicted_logits, dim=1)\n",
        "top5 = torch.topk(predicted_probs, 5)\n",
        "\n",
        "print(\"\\n[CBOW ÏòàÏ∏° ÌÖåÏä§Ìä∏ Í≤∞Í≥º]\")\n",
        "print(f\"Î¨∏Îß•(Context): {[idx_to_word[idx] for idx in random_context]}\")\n",
        "print(f\"Ï†ïÎãµ(Center): {idx_to_word[true_center]}\")\n",
        "\n",
        "print(\"\\nTop-5 ÏòàÏ∏° Îã®Ïñ¥:\")\n",
        "for i in range(5):\n",
        "    idx = top5.indices[0][i].item()\n",
        "    prob = top5.values[0][i].item()\n",
        "    print(f\"{i+1}. {idx_to_word[idx]} (ÌôïÎ•†: {prob:.4f})\")\n",
        "\n",
        "# 16. ÎØ∏Îûò Ïã†Ï°∞Ïñ¥ ÏòàÏ∏° Î∞è ÏÉùÏÑ±\n",
        "\n",
        "# 3Í∞ú ÎûúÎç§ Ï∂îÏ∂ú\n",
        "sample_rows = df.sample(3, random_state=random.randint(0,10000))\n",
        "\n",
        "print(\"\\n[ÎØ∏Îûò Ïã†Ï°∞Ïñ¥ Ïä§ÌÉÄÏùº ÏòàÏ∏° Î∞è ÏÉàÎ°úÏö¥ Ïã†Ï°∞Ïñ¥ ÏÉùÏÑ±]\")\n",
        "selected_words = []\n",
        "combined_styles = set()\n",
        "\n",
        "for idx, row in sample_rows.iterrows():\n",
        "    word = row['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥']\n",
        "    style = row['Ïä§ÌÉÄÏùº Ìå®ÌÑ¥']\n",
        "    print(f\"Í∏∞Ï°¥ Ïã†Ï°∞Ïñ¥: {word}\")\n",
        "    print(f\"Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: {style}\\n\")\n",
        "    selected_words.append(word)\n",
        "    combined_styles.update(style.split(', '))\n",
        "\n",
        "# Ïã†Ï°∞Ïñ¥ ÏÉùÏÑ± (Ïïû, Ï§ëÍ∞Ñ, ÎÅù Ï°∞Ìï©)\n",
        "def safe_pick(word, pos):\n",
        "    if len(word) == 1:\n",
        "        return word\n",
        "    if pos == 'start':\n",
        "        return word[0]\n",
        "    elif pos == 'middle':\n",
        "        return word[len(word)//2]\n",
        "    elif pos == 'end':\n",
        "        return word[-1]\n",
        "    else:\n",
        "        return random.choice(word)\n",
        "\n",
        "new_created_word = (\n",
        "    safe_pick(selected_words[0], 'start') +\n",
        "    safe_pick(selected_words[1], 'middle') +\n",
        "    safe_pick(selected_words[2], 'end')\n",
        ")\n",
        "\n",
        "print(f\"üëâ ÏÉùÏÑ±Îêú ÎØ∏Îûò Ïã†Ï°∞Ïñ¥: {new_created_word}\")\n",
        "print(f\"üëâ ÏòàÏÉÅÎêòÎäî Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: {', '.join(list(combined_styles))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gVj6sSPS7ZY",
        "outputId": "d322a182-fc50-4604-f655-17974ada9620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 103.0230\n",
            "Epoch [2/20], Loss: 99.9260\n",
            "Epoch [3/20], Loss: 97.1296\n",
            "Epoch [4/20], Loss: 94.3229\n",
            "Epoch [5/20], Loss: 91.5756\n",
            "Epoch [6/20], Loss: 88.7025\n",
            "Epoch [7/20], Loss: 86.0574\n",
            "Epoch [8/20], Loss: 83.2617\n",
            "Epoch [9/20], Loss: 80.4505\n",
            "Epoch [10/20], Loss: 77.7155\n",
            "Epoch [11/20], Loss: 74.8336\n",
            "Epoch [12/20], Loss: 72.2546\n",
            "Epoch [13/20], Loss: 69.4565\n",
            "Epoch [14/20], Loss: 66.6130\n",
            "Epoch [15/20], Loss: 63.9407\n",
            "Epoch [16/20], Loss: 61.2313\n",
            "Epoch [17/20], Loss: 58.5000\n",
            "Epoch [18/20], Loss: 55.9597\n",
            "Epoch [19/20], Loss: 53.3505\n",
            "Epoch [20/20], Loss: 50.5186\n",
            "\n",
            "[CBOW ÏòàÏ∏° ÌÖåÏä§Ìä∏ Í≤∞Í≥º]\n",
            "Î¨∏Îß•(Context): ['ÎãâÎÑ§ÏûÑ']\n",
            "Ï†ïÎãµ(Center): Ï∞®Î≥Ñ\n",
            "\n",
            "Top-5 ÏòàÏ∏° Îã®Ïñ¥:\n",
            "1. Ï∞®Î≥Ñ (ÌôïÎ•†: 0.0729)\n",
            "2. ÎÑàÎ¨¥ (ÌôïÎ•†: 0.0090)\n",
            "3. Ï†äÏùÄÏù¥Îì§ (ÌôïÎ•†: 0.0066)\n",
            "4. Î∂ÄÎã¥Îßå (ÌôïÎ•†: 0.0059)\n",
            "5. Í∏∞ÎÉ• (ÌôïÎ•†: 0.0053)\n",
            "\n",
            "[ÎØ∏Îûò Ïã†Ï°∞Ïñ¥ Ïä§ÌÉÄÏùº ÏòàÏ∏° Î∞è ÏÉàÎ°úÏö¥ Ïã†Ï°∞Ïñ¥ ÏÉùÏÑ±]\n",
            "Í∏∞Ï°¥ Ïã†Ï°∞Ïñ¥: ÎπµÌîÑ\n",
            "Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: ÏßßÏùå\n",
            "\n",
            "Í∏∞Ï°¥ Ïã†Ï°∞Ïñ¥: Íæ∏ÏóêÏóë\n",
            "Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: ÏßßÏùå\n",
            "\n",
            "Í∏∞Ï°¥ Ïã†Ï°∞Ïñ¥: ÏÇ¨Î∞îÏÇ¨\n",
            "Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: ÏßßÏùå\n",
            "\n",
            "üëâ ÏÉùÏÑ±Îêú ÎØ∏Îûò Ïã†Ï°∞Ïñ¥: ÎπµÏóêÏÇ¨\n",
            "üëâ ÏòàÏÉÅÎêòÎäî Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: ÏßßÏùå\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ÏµúÏ¢ÖÎ≥∏"
      ],
      "metadata": {
        "id": "INeVoyEfUPw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "\n",
        "# 1. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
        "df = pd.read_csv('/content/Ïã†Ï°∞Ïñ¥_Ïä§ÌÉÄÏùº_Ìå®ÌÑ¥_ÏôÑÏÑ±Î≥∏_UTF8SIG.csv')\n",
        "\n",
        "# 2. Ïã†Ï°∞Ïñ¥ Î≤àÌò∏ Ï†úÍ±∞ Ìï®Ïàò\n",
        "def clean_word(word):\n",
        "    word = str(word)\n",
        "    return re.sub(r'^\\d+\\.', '', word).strip()\n",
        "\n",
        "df['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'] = df['Ïã†Ï°∞Ïñ¥'].apply(clean_word)\n",
        "\n",
        "# 3. ÎûúÎç§ 1Í∞ú Ïã†Ï°∞Ïñ¥ ÎΩëÍ∏∞\n",
        "first_sample = df.sample(1, random_state=random.randint(0,10000)).iloc[0]\n",
        "first_word = first_sample['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥']\n",
        "first_style = first_sample['Ïä§ÌÉÄÏùº Ìå®ÌÑ¥']\n",
        "\n",
        "print(f\"\\n[Step 1] ÎûúÎç§ÏúºÎ°ú ÎΩëÏùÄ Ïã†Ï°∞Ïñ¥\")\n",
        "print(f\"Ïã†Ï°∞Ïñ¥: {first_word}\")\n",
        "print(f\"Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: {first_style}\")\n",
        "\n",
        "# 4. ÎπÑÏä∑Ìïú Ïä§ÌÉÄÏùº Ìå®ÌÑ¥ Í∞ÄÏßÑ Ïã†Ï°∞Ïñ¥ Ï∞æÍ∏∞\n",
        "# (Î∂ÄÎ∂Ñ Îß§Ïπ≠ Ìè¨Ìï®)\n",
        "similar_words_df = df[df['Ïä§ÌÉÄÏùº Ìå®ÌÑ¥'].str.contains(first_style.split(',')[0].strip())]\n",
        "\n",
        "# Ï≤´ Î≤àÏß∏Î°ú ÎΩëÏùÄ Îã®Ïñ¥Îäî Ï†úÏô∏\n",
        "similar_words_df = similar_words_df[similar_words_df['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'] != first_word]\n",
        "\n",
        "# 5. Í∑∏ Ï§ë 2Í∞ú ÎûúÎç§ÏúºÎ°ú Ï∂îÍ∞Ä Ï∂îÏ∂ú\n",
        "additional_samples = similar_words_df.sample(2, random_state=random.randint(0,10000))\n",
        "\n",
        "selected_words = [first_word] + additional_samples['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'].tolist()\n",
        "selected_styles = [first_style] + additional_samples['Ïä§ÌÉÄÏùº Ìå®ÌÑ¥'].tolist()\n",
        "\n",
        "print(f\"\\n[Step 2] ÎπÑÏä∑Ìïú Ïä§ÌÉÄÏùº Ìå®ÌÑ¥ÏùÑ Í∞ÄÏßÑ Ï∂îÍ∞Ä Ïã†Ï°∞Ïñ¥ 2Í∞ú ÎΩëÍ∏∞\")\n",
        "for word, style in zip(selected_words[1:], selected_styles[1:]):\n",
        "    print(f\"Ïã†Ï°∞Ïñ¥: {word}, Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: {style}\")\n",
        "\n",
        "# 6. Ïã†Ï°∞Ïñ¥ ÏÉùÏÑ± Î°úÏßÅ\n",
        "def safe_pick(word, pos):\n",
        "    if len(word) == 1:\n",
        "        return word\n",
        "    if pos == 'start':\n",
        "        return word[0]\n",
        "    elif pos == 'middle':\n",
        "        return word[len(word)//2]\n",
        "    elif pos == 'end':\n",
        "        return word[-1]\n",
        "    else:\n",
        "        return random.choice(word)\n",
        "\n",
        "new_created_word = (\n",
        "    safe_pick(selected_words[0], 'start') +\n",
        "    safe_pick(selected_words[1], 'middle') +\n",
        "    safe_pick(selected_words[2], 'end')\n",
        ")\n",
        "\n",
        "# 7. ÏµúÏ¢Ö Ï∂úÎ†•\n",
        "print(f\"\\n[Step 3] üëâ 3Í∞ú Îã®Ïñ¥Î°ú ÏÉùÏÑ±Îêú ÎØ∏Îûò Ïã†Ï°∞Ïñ¥: {new_created_word}\")\n",
        "combined_styles = set()\n",
        "for style in selected_styles:\n",
        "    combined_styles.update(style.split(', '))\n",
        "print(f\"üëâ ÏòàÏÉÅÎêòÎäî Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: {', '.join(list(combined_styles))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMFdWWF-ClNl",
        "outputId": "c1380ce6-84ec-4f07-b9ae-1f39c6b30e96"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Step 1] ÎûúÎç§ÏúºÎ°ú ÎΩëÏùÄ Ïã†Ï°∞Ïñ¥\n",
            "Ïã†Ï°∞Ïñ¥: ÌïëÌîÑ\n",
            "Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: ÏùåÏÑ±Ï†ÅÌëúÌòÑ\n",
            "\n",
            "[Step 2] ÎπÑÏä∑Ìïú Ïä§ÌÉÄÏùº Ìå®ÌÑ¥ÏùÑ Í∞ÄÏßÑ Ï∂îÍ∞Ä Ïã†Ï°∞Ïñ¥ 2Í∞ú ÎΩëÍ∏∞\n",
            "Ïã†Ï°∞Ïñ¥: ÏñÄÎç∞Î†à, Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: ÏùåÏÑ±Ï†ÅÌëúÌòÑ\n",
            "Ïã†Ï°∞Ïñ¥: Ìå©Ìè≠, Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: ÏùåÏÑ±Ï†ÅÌëúÌòÑ\n",
            "\n",
            "[Step 3] üëâ 3Í∞ú Îã®Ïñ¥Î°ú ÏÉùÏÑ±Îêú ÎØ∏Îûò Ïã†Ï°∞Ïñ¥: ÌïëÎç∞Ìè≠\n",
            "üëâ ÏòàÏÉÅÎêòÎäî Ïä§ÌÉÄÏùº Ìå®ÌÑ¥: ÏùåÏÑ±Ï†ÅÌëúÌòÑ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Îã§ÏùåÏùÄ TF-IDF Ïú†Ìñâ Í∞ÄÎä•ÏÑ± ÏòàÏ∏°"
      ],
      "metadata": {
        "id": "Rl3dsZrzWy28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ TF-IDF Ïã§Ìóò Ï¥àÍ∏∞Ìôî\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import re\n",
        "import random\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
        "train_df = pd.read_csv('/content/Ïã†Ï°∞Ïñ¥_Îç∞Ïù¥ÌÑ∞ÏÖã_Ïú†Ìñâ.csv')\n",
        "predict_df = pd.read_csv('/content/Ïã†Ï°∞Ïñ¥_Îç∞Ïù¥ÌÑ∞ÏÖã_ÏòàÏ∏°.csv')"
      ],
      "metadata": {
        "id": "2dTQCfO2Wujf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ TF-IDF Îç∞Ïù¥ÌÑ∞ Ï†ïÏ†ú\n",
        "\n",
        "def clean_word(word):\n",
        "    word = str(word)\n",
        "    return re.sub(r'^\\d+\\.', '', word).strip()\n",
        "\n",
        "train_df['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'] = train_df['Ïã†Ï°∞Ïñ¥'].apply(clean_word)\n",
        "predict_df['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'] = predict_df['Ïã†Ï°∞Ïñ¥'].apply(clean_word)"
      ],
      "metadata": {
        "id": "dlDymg86XlZZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ TF-IDF Î≤°ÌÑ∞Ìôî\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=300)\n",
        "X_train = vectorizer.fit_transform(train_df['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'].fillna(''))\n",
        "labels = train_df['Ïú†ÌñâÏó¨Î∂Ä']"
      ],
      "metadata": {
        "id": "z4OggBpnXz0H"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Î°úÏßÄÏä§Ìã± ÌöåÍ∑Ä Î™®Îç∏ ÌïôÏäµ\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "DupTZytwYBU9",
        "outputId": "9f7f32b0-2bb0-4bed-a52c-7adfd39c14ff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"‚ñ∏\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"‚ñæ\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ ÏòàÏ∏° Îç∞Ïù¥ÌÑ∞ Î≤°ÌÑ∞ Î≥ÄÌôò\n",
        "\n",
        "X_predict = vectorizer.transform(predict_df['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'].fillna(''))"
      ],
      "metadata": {
        "id": "NzydxjwkYQw0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Ïú†Ìñâ Ïó¨Î∂Ä ÏòàÏ∏°\n",
        "\n",
        "predict_preds = model.predict(X_predict)\n",
        "\n",
        "for word, pred in zip(predict_df['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'], predict_preds):\n",
        "    print(f\"Ïã†Ï°∞Ïñ¥: {word} -> ÏòàÏ∏° Í≤∞Í≥º: {'Ïú†ÌñâÌï† Í≤É' if pred == 1 else 'ÎπÑÏú†ÌñâÌï† Í≤É'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCXggdYSYgCk",
        "outputId": "01a671f1-44e6-4da2-d9f9-a5227963664c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ïã†Ï°∞Ïñ¥: Ï†ïÎàÑÎßà -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎØ∏ÏπúÎÖÑ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎØ∏ÏπúÎÑò -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î∞úÏª® -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏñµÌÖê -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ï§ëÍ∫ΩÎßà -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Í∞ìÏÉù -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ïù¥ÏÑ†Ï¢å -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎÑ§Ïπ¥ÎùºÏø†Î∞∞ÎãπÌÜ† -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎßâÎÇòÍ∑Ä -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏÑπÏãúÌë∏Îìú -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Í∞ÄÎ©¥ÎπÑ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ï∂îÍµ¨ÎØ∏ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏïàÎÇ®ÎØ∏ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏùµÏÜç -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ï∫òÎ∞ï -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î∂ÑÍπ®ÎØ∏ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎèºÏßÄÎü∞ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Íæ∏ÏóêÏóë -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Í∞êÎã§ÏÇ¥ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Í∞êÎã§Îí§ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ï¢ÖÎÖ∏ÌîåÏòà -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏãπÏãπ ÍπÄÏπò -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏùºÏÑ∏Ïä§ÏΩî -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÌïúÌîåÎ£®Ïñ∏ÏÑú -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏúÑÏë§ÏãúÍ∞ú -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î∞•ÌîåÎ¶≠Ïä§ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎûúÏÑ†ÏÉùÎãò=ÎûúÏÑ†ÏÉÅÎãò=ÎûúÏÑ†ÏÉù=ÎûúÏúÑÏù∏ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏàòÎ∞úÏÉàÎÅº -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏàòÎ∞úÎÖÑ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î¨¥ÏßÄÏª¨ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏåçÎ¨¥ÏßÄÏª¨ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏÜêÏ†àÎØ∏ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÌÖåÎ¨¥ Ïù∏Í∞Ñ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎäêÏ¢ã -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÍπäÏÉù -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏÑ∏ÏÉÅÏù¥ ÎÇ† ÏñµÍπåÌï¥ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î∂àÏÜå -> ÏòàÏ∏° Í≤∞Í≥º: Ïú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î∞òÏã† -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î∞òÎ∞ï -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ïú∞Ï∞® -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎãâÏ∞® -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏÑ§Ï∞∏ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ï†ÑÍ≥µ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ïã´ÌÖå -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏûÑÍµ¨ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Íµ¨Ï∑® -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏÇ¨Î∞îÏÇ¨ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Íæ∏ÏïàÍæ∏ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏûêÎßåÏ∂î -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÌïëÌîÑ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏòÅÍ≥† -> ÏòàÏ∏° Í≤∞Í≥º: Ïú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î≥ÑÎã§Ï§Ñ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ïò§Ï†ÄÏπòÍ≥† -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎßåÎ∞òÏûòÎ∂Ä -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ïò§ÎÜÄÏïÑÎÜà -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ïª§ÏóΩ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ï°¥Ïûò -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ï°¥Ïòà -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ï°∏Í∑Ä -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: tmi -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: tmt -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎùµÏûë -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏîπÏÉÅÌÉÄ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ìå©Ìè≠ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î†àÏïå -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ï≤†ÏªπÏ≤†Ïªπ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏôÄÍæ∏,Î©¥ÏÉÅ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎπºÎ∞ïÏ∫îÌä∏ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: „ÖÉ„ÖÇ„Öã„Öå „ÖÇ„ÖÇ„ÖÇ„Ñ± -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î≥µÏÑ∏Ìé∏ÏÇ¥ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏåâÏÜåÎ¶¨ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: mukbang -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏÇºÍ∑ÄÎã§ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎÑà Ïù∏ÏÑ± Î¨∏Ï†ú ÏûàÏñ¥? -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î®∏Î¶¨Î∂ÄÌÑ∞ Î∞úÎÅùÍπåÏßÄ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Í∞ëÎ∂ÑÏã∏ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î®∏ÏÑ†129 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Íµ≠Î£∞ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÏûêÎßåÏ∂î -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: asmr -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î†àÍ≤åÎÖ∏ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: jmt -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ï°¥ÎßõÌÉ± -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ï°∏ÎßõÎÇò -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎßêÏûáÎ™ª -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î¨∏Ï∞ê -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Í∞ëÌà≠ÌäÄ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Í∏àÏÇ¨Îπ† -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Í¥ÄÏ¢Ö -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Îñ°Í¥ÄÏ¢Ö -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î≥ºÎ∞âÏßÑ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎåïÎåïÏù¥ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ ÏòàÏ∏° Í≤∞Í≥º ÏÉòÌîåÎßÅ\n",
        "\n",
        "n_samples = 5\n",
        "sample_rows = predict_df.sample(n=n_samples, random_state=random.randint(0,10000))\n",
        "\n",
        "sample_texts = sample_rows['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'].fillna('')\n",
        "sample_features = vectorizer.transform(sample_texts)\n",
        "sample_preds = model.predict(sample_features)\n",
        "\n",
        "print(\"\\n[ÎØ∏Îûò Ïã†Ï°∞Ïñ¥ Ïú†Ìñâ Ïó¨Î∂Ä ÏòàÏ∏° Í≤∞Í≥º]\\n\")\n",
        "for word, pred in zip(sample_rows['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'], sample_preds):\n",
        "    print(f\"Ïã†Ï°∞Ïñ¥: {word} -> ÏòàÏ∏° Í≤∞Í≥º: {'Ïú†ÌñâÌï† Í≤É' if pred == 1 else 'ÎπÑÏú†ÌñâÌï† Í≤É'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOyRkztlZDU9",
        "outputId": "7f9ba40e-cbb6-4703-c6a7-6100fbc3dee7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ÎØ∏Îûò Ïã†Ï°∞Ïñ¥ Ïú†Ìñâ Ïó¨Î∂Ä ÏòàÏ∏° Í≤∞Í≥º]\n",
            "\n",
            "Ïã†Ï°∞Ïñ¥: Ï≤†ÏªπÏ≤†Ïªπ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î∂àÏÜå -> ÏòàÏ∏° Í≤∞Í≥º: Ïú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î®∏ÏÑ†129 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ï†ÑÍ≥µ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎØ∏ÏπúÎÖÑ -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ ÏòàÏ∏° ÌôïÎ•† Í∏∞Î∞ò ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï Ï§ÄÎπÑ\n",
        "\n",
        "sample_probs = model.predict_proba(sample_features)\n",
        "\n",
        "threshold = 0.6  # Í∏∞Î≥∏ 0.5 ‚Üí 0.6ÏúºÎ°ú Ïã§Ìóò\n",
        "for word, prob in zip(sample_rows['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'], sample_probs):\n",
        "    prediction = 1 if prob[1] >= threshold else 0\n",
        "    print(f\"Ïã†Ï°∞Ïñ¥: {word} -> ÌôïÎ•†: {prob[1]:.4f} -> ÏòàÏ∏° Í≤∞Í≥º: {'Ïú†ÌñâÌï† Í≤É' if prediction == 1 else 'ÎπÑÏú†ÌñâÌï† Í≤É'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHnDLorKZv14",
        "outputId": "d3ff4d62-0476-4ff6-e41b-76127c87855b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ïã†Ï°∞Ïñ¥: Ï≤†ÏªπÏ≤†Ïªπ -> ÌôïÎ•†: 0.4982 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î∂àÏÜå -> ÌôïÎ•†: 0.5975 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î®∏ÏÑ†129 -> ÌôïÎ•†: 0.3996 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ï†ÑÍ≥µ -> ÌôïÎ•†: 0.4982 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎØ∏ÏπúÎÖÑ -> ÌôïÎ•†: 0.4982 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Îã§ÏñëÌïú ÏûÑÍ≥ÑÍ∞í ÌÖåÏä§Ìä∏\n",
        "\n",
        "for threshold in [0.5, 0.6, 0.7]:\n",
        "    print(f\"\\n[ÏûÑÍ≥ÑÍ∞í {threshold} Ï†ÅÏö© Í≤∞Í≥º]\")\n",
        "    for word, prob in zip(sample_rows['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'], sample_probs):\n",
        "        prediction = 1 if prob[1] >= threshold else 0\n",
        "        print(f\"Ïã†Ï°∞Ïñ¥: {word} -> ÌôïÎ•†: {prob[1]:.4f} -> ÏòàÏ∏° Í≤∞Í≥º: {'Ïú†ÌñâÌï† Í≤É' if prediction == 1 else 'ÎπÑÏú†ÌñâÌï† Í≤É'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVrzoQzDaI8k",
        "outputId": "2b00bef7-df77-42fa-d6b9-02407a92e4a7"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ÏûÑÍ≥ÑÍ∞í 0.5 Ï†ÅÏö© Í≤∞Í≥º]\n",
            "Ïã†Ï°∞Ïñ¥: Ï≤†ÏªπÏ≤†Ïªπ -> ÌôïÎ•†: 0.4982 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î∂àÏÜå -> ÌôïÎ•†: 0.5975 -> ÏòàÏ∏° Í≤∞Í≥º: Ïú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î®∏ÏÑ†129 -> ÌôïÎ•†: 0.3996 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ï†ÑÍ≥µ -> ÌôïÎ•†: 0.4982 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎØ∏ÏπúÎÖÑ -> ÌôïÎ•†: 0.4982 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "\n",
            "[ÏûÑÍ≥ÑÍ∞í 0.6 Ï†ÅÏö© Í≤∞Í≥º]\n",
            "Ïã†Ï°∞Ïñ¥: Ï≤†ÏªπÏ≤†Ïªπ -> ÌôïÎ•†: 0.4982 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î∂àÏÜå -> ÌôïÎ•†: 0.5975 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î®∏ÏÑ†129 -> ÌôïÎ•†: 0.3996 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ï†ÑÍ≥µ -> ÌôïÎ•†: 0.4982 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎØ∏ÏπúÎÖÑ -> ÌôïÎ•†: 0.4982 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "\n",
            "[ÏûÑÍ≥ÑÍ∞í 0.7 Ï†ÅÏö© Í≤∞Í≥º]\n",
            "Ïã†Ï°∞Ïñ¥: Ï≤†ÏªπÏ≤†Ïªπ -> ÌôïÎ•†: 0.4982 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î∂àÏÜå -> ÌôïÎ•†: 0.5975 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Î®∏ÏÑ†129 -> ÌôïÎ•†: 0.3996 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: Ï†ÑÍ≥µ -> ÌôïÎ•†: 0.4982 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n",
            "Ïã†Ï°∞Ïñ¥: ÎØ∏ÏπúÎÖÑ -> ÌôïÎ•†: 0.4982 -> ÏòàÏ∏° Í≤∞Í≥º: ÎπÑÏú†ÌñâÌï† Í≤É\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Îç∞Ïù¥ÌÑ∞ Î∂àÍ∑†Ìòï Î¨∏Ï†ú Î∞úÍ≤¨\n",
        "# Ïú†Ìñâ Ïã†Ï°∞Ïñ¥ ÏàòÍ∞Ä Ìõ®Ïî¨ ÎßéÏïÑÏÑú Î°úÏßÄÏä§Ìã± ÌöåÍ∑ÄÍ∞Ä 1(Ïú†ÌñâÌï† Í≤É)Î°ú Ìé∏Ìñ•Îê† Í∞ÄÎä•ÏÑ± ÏûàÏùå\n",
        "# Ìñ•ÌõÑ Í∞úÏÑ† Î∞©Ìñ•: Ïñ∏ÎçîÏÉòÌîåÎßÅ, Ïò§Î≤ÑÏÉòÌîåÎßÅ, ÎòêÎäî Îã§Î•∏ Î™®Îç∏ Ï†ÅÏö© Í≥†Î†§"
      ],
      "metadata": {
        "id": "myat3wjNaYvI"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ÎùºÏù¥Î∏åÎü¨Î¶¨ Î∂àÎü¨Ïò§Í∏∞\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import re\n",
        "\n",
        "# 2. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
        "df = pd.read_csv('/content/Ïã†Ï°∞Ïñ¥_Îç∞Ïù¥ÌÑ∞ÏÖã_ÏµúÏ¢ÖÎ≥∏_ÎûúÎç§ÎùºÎ≤®ÎßÅ.csv')\n",
        "\n",
        "# 3. Ïã†Ï°∞Ïñ¥ Î≤àÌò∏ Ï†úÍ±∞ (Ïòà: \"57.Í±çÍ≥†\" -> \"Í±çÍ≥†\")\n",
        "def clean_word(word):\n",
        "    word = str(word)\n",
        "    return re.sub(r'^\\d+\\.', '', word).strip()\n",
        "\n",
        "df['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'] = df['Ïã†Ï°∞Ïñ¥'].apply(clean_word)\n",
        "\n",
        "# 4. ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏôÄ Î†àÏù¥Î∏î Ï§ÄÎπÑ\n",
        "texts = df['ÏÑ§Î™Ö'].fillna('')  # ÏÑ§Î™ÖÏù¥ ÏóÜÏúºÎ©¥ Îπà Î¨∏ÏûêÏó¥\n",
        "labels = df['Ïú†ÌñâÏó¨Î∂Ä']  # 0 or 1\n",
        "\n",
        "# 5. TF-IDF Î≤°ÌÑ∞Ìôî\n",
        "vectorizer = TfidfVectorizer(max_features=500)  # ÏµúÎåÄ 500Í∞ú Îã®Ïñ¥Î°ú Ï†úÌïú\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# 6. ÌïôÏäµÏö© Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ¶¨\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# 7. Î°úÏßÄÏä§Ìã± ÌöåÍ∑ÄÎ°ú ÌïôÏäµ\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 8. ÏòàÏ∏° Î∞è ÌèâÍ∞Ä\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n[TF-IDF + Î°úÏßÄÏä§Ìã± ÌöåÍ∑Ä] Ïú†Ìñâ Ïó¨Î∂Ä ÏòàÏ∏° Ï†ïÌôïÎèÑ: {accuracy:.4f}\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"ÎπÑÏú†Ìñâ(0)\", \"Ïú†Ìñâ(1)\"]))\n",
        "\n",
        "# 9. ÏòàÏãúÎ°ú 5Í∞ú Î¨∏Ïû•Ïóê ÎåÄÌï¥ Ïú†Ìñâ Ïó¨Î∂Ä ÏòàÏ∏° Ï∂úÎ†•\n",
        "sample_texts = df['ÏÑ§Î™Ö'].sample(5, random_state=42)\n",
        "\n",
        "print(\"\\n[ÏòàÏãú 5Í∞ú Ïã†Ï°∞Ïñ¥ ÏÑ§Î™ÖÏóê ÎåÄÌïú Ïú†Ìñâ Ïó¨Î∂Ä ÏòàÏ∏°]\")\n",
        "sample_features = vectorizer.transform(sample_texts)\n",
        "sample_preds = model.predict(sample_features)\n",
        "\n",
        "for text, pred in zip(sample_texts, sample_preds):\n",
        "    print(f\"ÏÑ§Î™Ö: {text[:30]}... -> ÏòàÏ∏°: {'Ïú†Ìñâ' if pred == 1 else 'ÎπÑÏú†Ìñâ'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hDdJ4aeeGsf",
        "outputId": "928ca0a8-d683-44e7-d9a6-20f129f8638c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TF-IDF + Î°úÏßÄÏä§Ìã± ÌöåÍ∑Ä] Ïú†Ìñâ Ïó¨Î∂Ä ÏòàÏ∏° Ï†ïÌôïÎèÑ: 0.6364\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      ÎπÑÏú†Ìñâ(0)       0.65      0.27      0.38        41\n",
            "       Ïú†Ìñâ(1)       0.63      0.90      0.74        58\n",
            "\n",
            "    accuracy                           0.64        99\n",
            "   macro avg       0.64      0.58      0.56        99\n",
            "weighted avg       0.64      0.64      0.59        99\n",
            "\n",
            "\n",
            "[ÏòàÏãú 5Í∞ú Ïã†Ï°∞Ïñ¥ ÏÑ§Î™ÖÏóê ÎåÄÌïú Ïú†Ìñâ Ïó¨Î∂Ä ÏòàÏ∏°]\n",
            "ÏÑ§Î™Ö: ÎßåÎÇòÏÑú Î∞òÍ∞ÄÏõå Ïûò Î∂ÄÌÉÅÌï¥... -> ÏòàÏ∏°: Ïú†Ìñâ\n",
            "ÏÑ§Î™Ö: Î∞úÎ°ú Ïª®Ìä∏Î°§ÌïòÎã§... -> ÏòàÏ∏°: Ïú†Ìñâ\n",
            "ÏÑ§Î™Ö: Ïù∏ÌÑ∞ÎÑ∑Ïóê ÏûàÎäî ÏÇ¨ÏßÑ ÎèôÏòÅÏÉÅ Îì±Ïùò ÏûêÎ£åÍ∞Ä ÏÇ≠Ï†úÎêòÏóàÍ±∞ÎÇò Í≤Ω... -> ÏòàÏ∏°: Ïú†Ìñâ\n",
            "ÏÑ§Î™Ö: ÏóÑÏ≤≠ÎÇòÍ≤å Ïû¨ÎØ∏ÏûàÏùå... -> ÏòàÏ∏°: Ïú†Ìñâ\n",
            "ÏÑ§Î™Ö: Ïó∞Ïù∏Ïù¥ ÎêúÏßÄ 22Ïùº ÎêòÎäîÎÇ†.... -> ÏòàÏ∏°: Ïú†Ìñâ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ÎùºÏù¥Î∏åÎü¨Î¶¨ Î∂àÎü¨Ïò§Í∏∞\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import re\n",
        "\n",
        "# 2. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
        "df = pd.read_csv('/content/Ïã†Ï°∞Ïñ¥_Îç∞Ïù¥ÌÑ∞ÏÖã_ÏµúÏ¢ÖÎ≥∏_ÎûúÎç§ÎùºÎ≤®ÎßÅ.csv')\n",
        "\n",
        "# 3. Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨: Ïã†Ï°∞Ïñ¥ Î≤àÌò∏ Ï†úÍ±∞\n",
        "def clean_word(word):\n",
        "    word = str(word)\n",
        "    return re.sub(r'^\\d+\\.', '', word).strip()\n",
        "\n",
        "df['ÌÅ¥Î¶∞ Ïã†Ï°∞Ïñ¥'] = df['Ïã†Ï°∞Ïñ¥'].apply(clean_word)\n",
        "\n",
        "# 4. Feature(ÏÑ§Î™Ö ÌÖçÏä§Ìä∏)ÏôÄ Label(Ïú†Ìñâ Ïó¨Î∂Ä) Ï§ÄÎπÑ\n",
        "texts = df['ÏÑ§Î™Ö'].fillna('')  # ÏÑ§Î™Ö ÏóÜÎäî Í≤ΩÏö∞ Îπà Î¨∏ÏûêÏó¥Î°ú ÎåÄÏ≤¥\n",
        "labels = df['Ïú†ÌñâÏó¨Î∂Ä']\n",
        "\n",
        "# 5. Î≤°ÌÑ∞Ìôî ÏãúÎèÑ: TF-IDF ÏÑ†ÌÉù Ïù¥Ïú†\n",
        "# Ïã§Ìóò: CountVectorizerÎ≥¥Îã§ TF-IDFÍ∞Ä ÎπàÎèÑ Ï§ëÏöîÎèÑÎ•º Î∞òÏòÅÌï† Ïàò ÏûàÏñ¥ Ï†ïÎ≥¥Îüâ ÏÜêÏã§ÏùÑ Ï§ÑÏùº Ïàò ÏûàÏùå\n",
        "vectorizer = TfidfVectorizer(max_features=500)  # ÏµúÎåÄ 500Í∞ú Îã®Ïñ¥Î°ú Ï†úÌïú\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# 6. Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
        "# Ï≤òÏùåÏóêÎäî random_state Í≥†Ï†ïÌï¥ÏÑú Ïã§ÌóòÌñàÏßÄÎßå, Îß§Î≤à ÎûúÎç§ÌïòÍ≤å ÏÑ±Îä• Î≥¥ÎäîÍ≤å ÎÇ´Îã§Í≥† ÌåêÎã® ‚Üí random_state Ï†úÍ±∞\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2)\n",
        "\n",
        "# 7. Î™®Îç∏ ÏÑ†ÌÉù: Î°úÏßÄÏä§Ìã± ÌöåÍ∑Ä\n",
        "# Ïã§Ìóò: Îã®Ïàú ÏÑ†Ìòï Î∂ÑÎ•ò Î¨∏Ï†úÏù¥Í∏∞ ÎïåÎ¨∏Ïóê SVMÎ≥¥Îã§Îäî Îπ†Î•¥Í≥† Ìï¥ÏÑù Í∞ÄÎä•Ìïú Logistic Regression ÏÑ†ÌÉù\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 8. ÏòàÏ∏° Î∞è ÌèâÍ∞Ä\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n[TF-IDF + Î°úÏßÄÏä§Ìã± ÌöåÍ∑Ä] Ïú†Ìñâ Ïó¨Î∂Ä ÏòàÏ∏° Ï†ïÌôïÎèÑ: {accuracy:.4f}\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"ÎπÑÏú†Ìñâ(0)\", \"Ïú†Ìñâ(1)\"]))\n",
        "\n",
        "# 9. ÏòàÏãú 5Í∞ú Î¨∏Ïû• ÏòàÏ∏°\n",
        "# Ïù¥Ï†ÑÏóêÎäî random_stateÎ°ú Í≥†Ï†ïÎêòÏóàÏßÄÎßå, Ïã§Ìóò Îã§ÏñëÏÑ± ÌôïÎ≥¥Î•º ÏúÑÌï¥ random_state ÏóÜÏù¥ random ÏÉòÌîåÎßÅ Ï†ÅÏö©\n",
        "sample_texts = df['ÏÑ§Î™Ö'].sample(5)\n",
        "\n",
        "print(\"\\n[ÏòàÏãú 5Í∞ú Ïã†Ï°∞Ïñ¥ ÏÑ§Î™ÖÏóê ÎåÄÌïú Ïú†Ìñâ Ïó¨Î∂Ä ÏòàÏ∏°]\")\n",
        "sample_features = vectorizer.transform(sample_texts)\n",
        "sample_preds = model.predict(sample_features)\n",
        "\n",
        "for text, pred in zip(sample_texts, sample_preds):\n",
        "    print(f\"ÏÑ§Î™Ö: {text[:30]}... -> ÏòàÏ∏°: {'Ïú†Ìñâ' if pred == 1 else 'ÎπÑÏú†Ìñâ'}\")\n",
        "\n",
        "# 10. ÏÑ±Ï∞∞ Î∞è Í∞úÏÑ†Ï†ê ÏΩîÎ©òÌä∏\n",
        "print(\"\\n[ÏÑ±Ï∞∞ Î∞è Í∞úÏÑ†Ï†ê]\")\n",
        "print(\"- ÌòÑÏû¨ TF-IDFÎ°ú ÏÑ§Î™ÖÎßå Î≤°ÌÑ∞ÌôîÌñàÎäîÎç∞, ÏÑ§Î™ÖÏù¥ ÏßßÍ±∞ÎÇò ÎπàÏïΩÌïú Í≤ΩÏö∞ ÏòàÏ∏° ÏÑ±Îä•Ïù¥ Îñ®Ïñ¥Ïßà Ïàò ÏûàÏùå\")\n",
        "print(\"- Ïú†ÌñâÏó¨Î∂ÄÎ•º Îçî Ïûò ÎßûÏ∂îÎ†§Î©¥ 'ÏÑ§Î™Ö + Ïä§ÌÉÄÏùº Ìå®ÌÑ¥'ÏùÑ featureÎ°ú Ìï®Íªò ÎÑ£Îäî ÌôïÏû• ÏãúÎèÑÍ∞Ä Í∞ÄÎä•Ìï®\")\n",
        "print(\"- ÎòêÎäî Ïã†Ï°∞Ïñ¥ ÏûêÏ≤¥Î•º subword ÏàòÏ§ÄÏúºÎ°ú Î∂ÑÏÑùÌï¥ ÌäπÏßïÏùÑ Ï∂îÍ∞ÄÌïòÎäî Í≤ÉÎèÑ Í∞ÄÎä•\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCBx2FgWfjvC",
        "outputId": "2db75999-c02a-4b19-9978-510b95d37638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TF-IDF + Î°úÏßÄÏä§Ìã± ÌöåÍ∑Ä] Ïú†Ìñâ Ïó¨Î∂Ä ÏòàÏ∏° Ï†ïÌôïÎèÑ: 0.5455\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      ÎπÑÏú†Ìñâ(0)       0.48      0.23      0.31        44\n",
            "       Ïú†Ìñâ(1)       0.56      0.80      0.66        55\n",
            "\n",
            "    accuracy                           0.55        99\n",
            "   macro avg       0.52      0.51      0.48        99\n",
            "weighted avg       0.53      0.55      0.50        99\n",
            "\n",
            "\n",
            "[ÏòàÏãú 5Í∞ú Ïã†Ï°∞Ïñ¥ ÏÑ§Î™ÖÏóê ÎåÄÌïú Ïú†Ìñâ Ïó¨Î∂Ä ÏòàÏ∏°]\n",
            "ÏÑ§Î™Ö: ÎØ∏ÏπòÎèÑÎ°ù ÌÅ¥Î¶≠... -> ÏòàÏ∏°: Ïú†Ìñâ\n",
            "ÏÑ§Î™Ö: ÏñºÍµ¥ 0Ï†ê... -> ÏòàÏ∏°: ÎπÑÏú†Ìñâ\n",
            "ÏÑ§Î™Ö: ÏûêÏó∞Ïä§Îü¨Ïö¥ ÎßåÎÇ®ÏùÑ Ï∂îÍµ¨... -> ÏòàÏ∏°: ÎπÑÏú†Ìñâ\n",
            "ÏÑ§Î™Ö: ÎàÑÍµ¨ Î¨ºÏñ¥Î≥∏ ÏÇ¨Îûå?... -> ÏòàÏ∏°: ÎπÑÏú†Ìñâ\n",
            "ÏÑ§Î™Ö: Î¨¥Ïä®ÏùºÏù¥Ïïº?... -> ÏòàÏ∏°: Ïú†Ìñâ\n",
            "\n",
            "[ÏÑ±Ï∞∞ Î∞è Í∞úÏÑ†Ï†ê]\n",
            "- ÌòÑÏû¨ TF-IDFÎ°ú ÏÑ§Î™ÖÎßå Î≤°ÌÑ∞ÌôîÌñàÎäîÎç∞, ÏÑ§Î™ÖÏù¥ ÏßßÍ±∞ÎÇò ÎπàÏïΩÌïú Í≤ΩÏö∞ ÏòàÏ∏° ÏÑ±Îä•Ïù¥ Îñ®Ïñ¥Ïßà Ïàò ÏûàÏùå\n",
            "- Ïú†ÌñâÏó¨Î∂ÄÎ•º Îçî Ïûò ÎßûÏ∂îÎ†§Î©¥ 'ÏÑ§Î™Ö + Ïä§ÌÉÄÏùº Ìå®ÌÑ¥'ÏùÑ featureÎ°ú Ìï®Íªò ÎÑ£Îäî ÌôïÏû• ÏãúÎèÑÍ∞Ä Í∞ÄÎä•Ìï®\n",
            "- ÎòêÎäî Ïã†Ï°∞Ïñ¥ ÏûêÏ≤¥Î•º subword ÏàòÏ§ÄÏúºÎ°ú Î∂ÑÏÑùÌï¥ ÌäπÏßïÏùÑ Ï∂îÍ∞ÄÌïòÎäî Í≤ÉÎèÑ Í∞ÄÎä•\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Ïã§Ìóò Í≤∞Í≥º Ìï¥ÏÑù\n",
        "# - ÏûÑÍ≥ÑÍ∞í Ï°∞Ï†ïÏóê Îî∞Îùº Ïú†Ìñâ/ÎπÑÏú†Ìñâ Î∂ÑÎ•òÍ∞Ä Îã§Î•¥Í≤å ÎÇòÌÉÄÎÇ®\n",
        "# - Í∏∞Î≥∏ 0.5 ÏûÑÍ≥ÑÍ∞íÏù¥ Í∞ÄÏû• ÏïàÏ†ïÏ†ÅÏù¥ÎÇò, Ïú†Ìñâ Ïã†Ï°∞Ïñ¥ ÏòàÏ∏° RecallÏùÑ ÎÜíÏù¥Î†§Î©¥ 0.6~0.7ÎèÑ Í≥†Î†§Ìï† Ïàò ÏûàÏùå\n",
        "# - Ìñ•ÌõÑ: Ïã†Ï°∞Ïñ¥ ÏûêÏ≤¥ ÌäπÏßï(Í∏∏Ïù¥, ÏùåÏö¥, Í∞êÏ†ïÏÑ± Îì±) feature Ï∂îÍ∞Ä ÏãúÎèÑÌï¥Î≥º Í≤É"
      ],
      "metadata": {
        "id": "nfA9xa0fa_n6"
      },
      "execution_count": 66,
      "outputs": []
    }
  ]
}